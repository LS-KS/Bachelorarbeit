\chapter{Kameragestützten Validierungsprozessen in der Lagerverwaltung}\label{KameragestützteInventur}

Aufgabe der Lagerzelle ist die automatische Ein- und Auslagerung von Bechern mit oder ohne Produkt.
Die Informationen die dafür übergeben werden oder entstehen müssen gespeichert werden um bei einer Produktbestellung wieder das korrekte Produkt auszugeben.
Das in \ref{Software} beschriebene Datenmodell erledigt genau das. Wird ein Becher von Ort A nach Ort B bewegt, 
passieren notwendige Änderungen am Datenmodell selbst. Fehler im Datenmodell können durch systematische Tests des relevanten Quellcodes entdeckt und behoben werden. 
Zu diesem Zweck sind die Tests \verb|test\_inventory| und \verb|test_inventoryController| implementiert. 
Die Tests sind jedoch auf das Datenmodell und seinen Controller beschränkt und ein \glq ubefugter \grq Zugriff kann in Python nicht unebdingt verhindert werden. 
Eine Weitere Fehlerquelle kann die Möglichkeit der manuellen Überschreibung des Lagerbestands sein oder gar ein manueller Eingriff in der Lagerzelle selbst. 

Um diese Fehler zu erkennen soll ein kameragestütztes Inventursystem entworfen werden. 
    \section {Konzepte}
    Grundidee ist, dass eine Kamera am Greifer des Roboters so montiert ist, dass beim Abfahren existierender Wegpunkte jede mögliche Lagerstelle überwacht werden kann. 
    Mögliche Lagerorte sind:
    \begin{itemize}
        \item Der mobile Roboter in der Entladestation (Ein Becher). 
        \item Der Kommissionstisch (Zwei Abstellmöglichkeiten für Paletten)
        \item Das Lager (18 mögliche Abstellorte für Paletten)
    \end{itemize}
    Zusätzlich kann ein Becher oder eine Palette temporär von einem Greifer bewegt werden. All Diese Orte sind im Datenmodell und im GUI berücksichtigt. 
    Auf der Steuerung IRC5 des Roboters sind Routinen programmiert, die die jeweiligen Becher/Palettenbewegungen implementieren.
    Sie werden vom \verb|ABBController| aus über den entsprechenden Service via TCP aufgerufen. 

    Wenn die Kamera mittig auf dem Greifer montiert ist, kann diese Kamera erkennen ob in einem Lagerort eine Palette oder ein Becher vorhanden ist. 
    Dazu müssen an den Bechern und an den Paletten eindeutige optische Marker angebracht sein. 
    Ein entsprechendes Signal zur Bildaufzeichnung und Bildauswertung kann erfolgen, wenn der Greifer die Position vor dem Lagerort erreicht hat. 
    Da es sich bei den Produkten um eine simulierte Getränkeproduktion handelt, können sie nicht so leicht erkannt werden und sind daher nicht gegenstand dieser Arbeit. 
    
    Eine Weitere Überlegung ist, dass es keinen Mehrwert bringt Paletten voneinander zu unterscheiden, da diese nur den Zweck hat bis zu zwei Becher umfallsicher zu halten. 

    Aus der persönlichen Beobachtung der Lagerzelle reifte die Idee, dass alle Paletten und die vorderen Becher durch eine Übersichtskamera erkannt werden können. 
    Dazu wird eine Kamera mit einer höheren Auflösung in die obere Ecke der Lagerzelle gehängt. Das Regal ist dann in einem relativ kleinen Bildausschnitt im oberen Bildbereich zu sehen. 

    Der Roboter verdeckt dabei dein unteren linken Rand des Regals, sodass die Lagerpositionen L13, L14, L15 nicht erkannt werden können. 
    Die Vermutung liegt nahe, dass die Positionen L14 und L15 auch erkannt werden, wenn der Roboterarm in eine geeignete Position fährt. 
    Hier kann bspw. ein Signal der Steuerung emittiert werden, wenn der Roboterarm gerade den Becher im mobilen Roboter anfährt. 

    
    \section {Auswahl der optischen Marker}

    Die Arbeit von \cite{Hübler2019} legt die Verwendung von arUco Markern nahe. 
    Laut seiner Arbeit werden die Marker auch dann noch erkannt, wenn der vom Kamerahersteller angegebene Mindestabstand unterschritten wird und das Bild unscharf ist. 
    Der Artikel auf stackoverflow legt nahe, dass die Markerdetection robust gegenüber Rotation, Verbiegung und Knicken der Marker ist. 
    Die Marker sind quadratisch, haben einen umlaufenden Rand und eine mittige Binärmatrix. 
    Über die Binärmatrix kann auch die Orientierung des Markers ermittelt werden.
    Da jede Binärmatrix einen Integerwert darstellt, ist die Anzahl der einzigartigen ID's aus den Binärmatritzen proportional zu ihrer Kantenlänge.
    
    --> Hier Größenverhältnisse der Marker und ID Bereich beschreiben und 6x6x250 begründen.

    \section {Auswahl der Kameras, Objektive und Aufstellung}

    Die Auswahl der Kameras beruht auf meiner Semesterarbeit. Die ausgewählten Kameras sind Industriekameras und haben keine OPtik. 
    Daher erfolgt anhand des kleinsten zu erwartenden Details, des Bildbereichs sowie der Kameraparameter (Brennweite und Sensorgröße) eine Objektivauswahl. 

    Für die Übersichtskamera wurde das Objektiv auf den größtmöglichen Schärfebereich eingestellt, sodass die Chance besteht eine Markererkennung sowohl im Regal als auch im KOmmissionstisch vorzunehmen. 

    ----

    Die ursprüngliche $\mu$Eye Kamera erweist sich mit Objektiv zu lang um sie an dem Greifer zu montieren ohne die Kollisionsgefahr zu erhöhen. 
    Aus diesem Grund wird versuchsweise eine ESP32 Kamera mit verbauter Optik verwendet. Diese hat einen eingebauten Webserver, über dessen RESTful API die aufgenommenen Bilder heruntergeladen werden können. 
    Die Kamera kann von ihren Abmaßen direkt auf den Greifer montiert werden und der angegebene Mindestabstand für die Appertur wird dabei nur knapp unterschritten. 
    Testaufnahmen zeigen, dass die Marker bei gegebenen Abstandsverhältnissen noch scharf abgebildet werden. 

    \begin{figure}
        \caption[Optische Verzerrung]
        {\small bei diesem Bild wurde eine Markererkennung mit dem OpenCV Paket durchgeführt, Aus den maximalen und minimalen Koordinaten wurde ein grünes Rechteck erstellt. Aus den 4 Markern der rechten Seite und den 2 Markern links wurde der Verlauf der vertikalen Regalkanten ermittelt und das Bild so gedreht, dass die rechte Seite vertikal ist.}\label{fig:figure10}
        \includegraphics[width = \textwidth ]{Bilder/verzerrtesBild.png}
        \centering
    \end{figure}

    \section {Perspektivische Korrektur und Marker Erkennung}

    Wie in dem Bild \ref{fig:figure10} zu sehen, ist das Regal perspektivisch stark verzerrt.
    Der Verzerrung durch die Linse wird vor Allem durch die horizontale Linie deutlich, nachdem das Bild so gedreht wurde, dass der rechte Regalrand vertikal ist. 
    Für die Erkennung der Marker und der Bildsegmentierung für die Zuordnung der Marker zu den Lagerorten ist es also erforderlich entweder eine geometrische Kalibrierung der Appertur vorzunehmen oder das Bild im interessierenden Bereich zu rektifizieren. 
    Eine 4-Punkt Entzerrung ist aufgrund des geringen Aufwands und der hohen Auflösung der gewählte Weg. 
    Um die Rektifizierung zu automatisieren, wurden die oberen Regalecken und die Kreuzungen der Regalböden mit dem äußeren Rahmen mit identischen arUco Marker versehen. 
    Die obere linke Regalecke ist nicht im Bildbereich und der dortige Marker kann somit nicht erkannt werden. 
    Als die Für die 4-Punkt Entzerrung notwendigen Punkte werden die Schnittpunkte der beiden oberen Regalböden mit dem Außenrand des Regals gewählt. 
    Die eigentliche Rektifizierung ist Teil des \verb|stocktaking| Service der Software und nutzt dazu die Methoden des \verb|skimage| Pakets in Python.

    \begin{figure}
        \caption[Ablauf Bildverarbeitung und Markererkennung der Übersichtskamera]
        {\small Die ursprüngliche Aufnahme wird dupliziert. Der obere Pfad zeigt die Ermittlung der Transformationsdaten mithilfe der Regalmarker. Im Unteren PFad wird das Originalbild rektifiziert und segmentiert bevor Paletten und Becher-ID's extrahiert werden.}\label{fig:figure11}
        \includegraphics[width = \textwidth ]{Bilder/AblaufErkennungSellCam.png}
        \centering
    \end{figure}

    Für die perspektivische Entzerrung wird das Python Paket \verb|skimage| verwendet. Der Name steht für \glq \textbf{S}cience \textbf{K}it for \textbf{Image}processing \grq.
    Das Untermodul \verb|util.transform| enthält eine Klasse \verb|ProjectiveTransform|. Die \verb|estimate| Methode dieser Klasse akzeptiert als Parameter zwei Matritzen \verb|src| und \verb|dst| und ermittelt eine Transformationsmatrix.
    Als \verb|src| werden die Bildkoordinaten der 4 Marker angegeben und als dst die \glq realen \grq Koordinaten.
    Die \verb|warp| Methode akzeptiert neben der Instanz der o.g. Klasse auch die Eingabe- und Ausgabedimensionen. 

    Versuche haben gezeigt, dass die Erkennungsrate der Regalmarker deutlich verbessert werden kann, wenn vor der Markererkennung die Helligkeit und der Kontrast angepasst werden. 
    Noch dazu tritt eine deutliche Verbesserung der Erkennungsrate ein, wenn das Bild etwas überbelichtet ist. 
    Leider sind die Erkennungsraten der Paletten/Becher deutlich schlechter, wenn das angepasste Bild verwendet wird, welches für die Erkennung der Regalmarker optimiert ist. 
    Die Lichtverhältnisse der einzelnen Lagerplätzen sind unterschiedlich, weshalb es naheliegt den Kontrast und die Helligkeit der einzelnen Bildbereiche anzupassen. 
    Dafür wird nach der Bildaufnahme das Bild mit der \verb|copy| Funktion des \verb|numpy| Pakets dupliziert.

    Nach der Transformation des Bildes, wird das originale Bild auf die relevanten Bereiche zugeschnitten und in die jeweiligen Lagerbereiche segmentiert.
    Die einzelnen Lagerbereiche \glqq sections \grqq werden danach noch mal zwischen Palette und Becher Bereiche getrennt. 
    Danach erfolgt erneut eine Helligkeits und Kontrastanpassung bevor die einzelnen Marker in den Bildsegmenten bestimmt werden. 
    Die Bildsegmente und die erkannten Marker mit ihren ID's werden in entsprechenden Listen des \verb|stocktaking| Services gespeichert.

    \section {Automatische Kontrast und Helligkeitsanpassung}
    Wie in dem vorangegangenen Abschnitt bereits erwähnt, müssen Bilder oder einzelne Bildsegmente einer adaptiven Kontrast- und Helligkeitsanpassung unterzogen werden. 
    Dies geschieht in der Methode \verb|_automatic_brightness_and_contrast| des \verb|stocktaking| Service. 
    Die Methode bekommt als Argumente ein Bild oder Bildausschnitt als Graustufenbild, welches mit der Bildklasse des \verb|cv2| paket (OpenCV) identisch ist. 
    Außerdem wird ein Integer als ganzzahliger Prozentwert übergeben.
    Der Ablauf ist dann wie folgt: 
    \begin{itemize}
        \item Von dem übergebenen Bildausschnitt wird ein Histogramm der Graustufenskala mit einer Auflösung von 8 Bit berechnet. Dazu wird die Funktion \verb|cv2.calcHist()| verwendet.
        \item Die kumulative Verteilung des Histogramms wird berechnet.
        \item Vom unteren und oberen Rand werden so lange Werte entfernt, bis der übergebene Prozentwert überschritten ist. 
        \item Aus dem beschnittenen Histogramm werden der $\alpha$ und $\beta$ Werte für die Skalierung zu dem neuen Histogramm berechnet. 
        \item Mit den berechneten Werten für $\alpha$ und $\beta$ wird die \verb|cv2.ScaleHistAbs()| Funktion ausgeführt und so ein neues Histogramm aus den alten Werten erzeugt. 
    \end{itemize}


    \section {Markererkennung}

    Für die Erkennung der optischen arUco Marker wird aus der OpenCV Bibliothek das Modul \verb|aruco| benutzt. Die Erkennung erfolgt über die Funktion \verb|detectMarkers|.
    Auf Grundlage eines mitgegebenen arUco-Dictionarys und einem Bild oder Bildausschnitt, erfolgt eine Erkennung. 
    Der Algorhythmus und die dabei verwendeten Parameter sind in \cite{OpenCVaruco} beschrieben.
    Bei der Erkennung wird das Bild zunächst in ein binärwertiges Bild umgewandelt und anschließend nach rechteckartigen Merkmalen durchsucht. 
    Für jedes gefundene Merkmal wird die Breite des schwarzen Rahmens um die Binärmatrix ermittelt. 
    Standardmäßig entspricht die Breite des Rahmens genau einem Bit.
    Nach einer Rektifizierung wird der Bildausschnitt des Markers mit Hilfe eines Gitters in eine Binärmatrix unterteilt und decodiert. 
    Die Eckpunkte fehlerhaft decodierter oder vorangehend als nicht erkannte Marker abgelehnte Merkmale werden in der Liste \verb|rejected| ausgegeben.
    Die Ecken und ID's der erkannten Marker werden in der Liste \verb|corners| bzw. \verb|ids| ausgegeben.
    Ob die Erkennung der Marker erfogreich ist, hängt also unter Anderem davon ab das die einzelnen Bits in der Binärmatrix ähnlich breit sind. 
    
    Der Algorhytmus der Markerekennung benutzt für die einzelnen Schritte in Summe 31 Parameter, die viele Aspekte der Erkennung beeinflussen.
    Die Standardeinstellungen sind für Bilder / Bildsegmente moderater Größe eine sinnvolle Wahl. Die Erkennung versagt allerdings, wenn wie in 
    \ref{fig:figure20} der Marker durch eine Wölbung verzerrt wird, die in diesem Falle durch die 4-Punkt Verzerrung überlagert ist.
    In dem Bild ist deutlich zu erkennen, dass die Bits im rechten Bereich deutlich breiter als im linken Bereich sind. 
    Da die Marker nicht konsistent gleich angebracht sind und die Transportprozesse nicht verhindern, dass die Becher sich geringfügig drehen, ist eine Kompensation mit einem Filter 
    schwierig. 

    \subsection{Bildmatrixanalyse an einem gewölbten Marker}

    In ref{fig:figure21} ist die Binärmatrix eines durch die Wölbung verzerrten Markers dargestellt. In \ref{Bildmatrixanalyse} ist das Script aufgeschrieben mit dem die Abbildung erzeugt wurde.
    Das Originalsegment mit dem Bechermarker wurde grob auf die Außenmaße des Markers zugeschnitten. 
    Mit der Schwellenwert-Funktion wird das Graubildsegment zu einem binärwertigem Bild umgewandelt.
    Durch die vorangegangene Helligkeits- und Kontrastanpassung, können die Grenzen dafür sehr eng gefasst werden.
    Anschließend wird über zwei Listen die x- bzw. y- Koordinaten der Bitmatrix angegeben. 
    Die Koordinaten der beiden Listen sind manuell ermittelt und zwar so, dass die Linien der Bitgrenzen immer in einem Bit Bereich liegen.
    Die Linien der Bitgrenzen werden mit einer eigenen for-Schleife gezeichnet. Ist der Wert des aktuellen Pixels schwarz (also alle Farbkanäle sind 0), wird der Pixel übersprungen. 
    Dadurch entsteht der Eindruck, dass die Linien hinter dem Marker verlaufen und die Erscheinung des Markers wird nicht gestört. 
    In dem mittleren Unterplot sind die Koordinaten der Linien der Bitmatrix über den resultierenden Bits aufgetragen. 
    Ein Bit ist dabei das eingeschlossene Rechteck zwischen der jeweilgen x- bzw. y- Linie. Außerdem wurde für jeden Plot ein ausgleichspolynom geschätzt, um die unterschiedlichen Verläufe zu zeigen. 
    Im dritten Plot rechts, ist die Änderung der Pixelbreite eines Bits in x- bzw. y- Richtung über die Bitnummer aufgetragen.
    Hier zeigt sich, dass in der y-Richtung bis auf einem Ausreißer keine Änderung auftritt. In x-Richtung ändert sich die Breite der Bits jedoch deutlich.
    Beides ist wiederum mit je einem Ausgleichspolynom dargestellt. Hier ist anzumerken, dass der jeweils erste Datentupel nicht für das Ausgleichspolynom verwendet wurde. 

    Vergleicht man den Marker mit dem aus den resultierenden Gitter und bedenkt, dass der schwarze Rand um die eigentliche 6x6 Bitmatrix die Funktion hat, die Größe des Markers und die größe eines einzelnen Bits zu berechnen, 
    ist es nicht verwunderlich, dass die Erkennung fehlschlägt. Die ersten beiden Bits in x-Richtung (letzten in y-Richtung) haben ohne weitere perspektivische Optimierung einen anderen Wert als der Rest des Rands. 
    Der Algorhythmus wird daher den Marker ablehnen, wie das in der Praxios auch der Fall ist.

    \begin{figure}
        \caption[Bildaufnahme aus der Lagerzelle von der Übersichtskamera.]
        {\small Die Abbildung zeigt die Aufnahme der Kamera und Appertur ohne weitere Bildverarbeitung außerhalb der Kamera. Die leichte Überbelichtung begünstigt das Erkennen der Regalmarker.}\label{fig:figure12}
        \includegraphics[width = \textwidth]{Bilder/LZIP_raw_shot.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bildaufnahme aus der Lagerzelle nach der Kontrastanpassung.]
        {\small Die Aufnahme aus Bild \ref{fig:figure12} nach der automatischen Helligkeits und Kontrastanpassung.}\label{fig:figure13}
        \includegraphics[width = \textwidth]{Bilder/LZ_Kontrastanpassung.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bild aus der Lagerzelle nach der Rektifizierung und Zuschnitt auf relevanten Regalbereich]
        {\small Die Aufnahme aus Bild \ref{fig:figure12} nach der Kontrastanpassung und Rektifizierung. Im Vordergrund sind die Paletten und Becher gut zu erkennen. Die Becher in der zweiten Reihe sind jedoch deutlich verzerrt.}\label{fig:figure14}
        \includegraphics[width = \textwidth]{Bilder/LZ_transformed.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bildzuschnitt einer Lagersektion und weitere Unterteilung in die Bereiche für Becher und Palette.]
        {\small Der Bildausschnitt zeigt einen Lagerslot mit Palette aber ohne Becher im vorderen Lagerslot, der aus \ref{fig:figure15} extrahiert wurde. Der Marker hat kein weißes Rechteck um sich was bedeutet, dass dieser Marker nicht erkannt wurde. Dies liegt vermutlich an der durch die Wölbung gestörten Binärmatrix}\label{fig:figure15}
        \includegraphics*[width = \textwidth/3]{Bilder/section_16.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bildzuschnitt einer Lagersektion und weitere Unterteilung in die Bereiche für Becher und Palette.]
        {\small Der Bildausschnitt zeigt einen Lagerslot mit Palette und Becher im vorderen Lagerslot, der aus \ref{fig:figure15} extrahiert wurde. Der Marker hat kein weißes Rechteck um sich was bedeutet, dass dieser Marker nicht erkannt wurde}\label{fig:figure18}
        \includegraphics[width = \textwidth/3]{Bilder/section_5.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bild aus der Lagerzelle nach der Rektifizierung und Zuschnitt auf einen Bereich einer Palette]
        {\small Bereich einer Palette, aus \ref{fig:figure14} extrahiert. Der dicke weiße Rahmen wird im Programmcode erzeugt wenn der Marker erfolgreich erkannt wurde. }\label{fig:figure16}
        \includegraphics[width = \textwidth/3]{Bilder/pallet_5.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bild aus der Lagerzelle nach der Rektifizierung und Zuschnitt auf einen Bereich einer Palette]
        {\small Bereich einer Palette, aus \ref{fig:figure14} extrahiert. Im Gegensatz zu \ref{fig:figure16} wurde der Marker nicht erkannt. }\label{fig:figure19}
        \includegraphics[width = \textwidth/3]{Bilder/pallet_3.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bild aus der Lagerzelle nach der Rektifizierung und Zuschnitt auf einen Bereich eines Bechers]
        {\small Bereich eines Bechers, aus \ref{fig:figure14} extrahiert. Deutlich sichtbar ist die durch die Wölbung des Bechers gestörte Binärmatrix des Markers.}\label{fig:figure17}
        \includegraphics[width = \textwidth/3]{Bilder/cup17.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Bild aus der Lagerzelle nach der Rektifizierung und Zuschnitt auf einen Bereich eines Bechers]
        {\small Bereich eines Bechers, aus \ref{fig:figure14} extrahiert. Auch hier ist die durch die Wölbung induzierte Verzerrung des Markers deutlich sichtbar.}\label{fig:figure20}
        \includegraphics[width = \textwidth/3]{Bilder/cup2.png}
        \centering
    \end{figure}

    
    \begin{figure}
        \caption[Bildmatrixanalyse]{\small Bildmatrixanalyse am verzerrten optischen Marker.}\label{fig:figure21}
        \includegraphics[width = \textwidth]{Bilder/cup6_analyse.png}
        \centering
    \end{figure}

    \clearpage
    \lstinputlisting[language=Python, caption=Bildmatrixanalyse (Dateipfade gekürzt), label=Bildmatrixanalyse]{Listings/Bildmatrixanalyse_Becher6.py}

    \subsection{Analyse der Erkennungsrate der Becher mittels Übersichtskamera}

    Die Bilder \ref{fig:figure12} bis \ref{fig:figure20} zeigen die Ergebnisse der Bildverarbeitungskette von der Aufnahme bis zur Bildsegmentierung und Erkennung.
    Die Bilder \ref{fig:figure22} bis \ref{fig:figure27} zeigen Überlegungen wie die Erkennung der Becher besser gelingen kann. 
    Das Problem der Wölbung wird durch die 4-Punkt Verzerrung und durch die geringe Pixelanzahl der relevanten Bildsegmente verschärft. 
    Ein Bit besteht nur noch aus wenigen Pixeln. 
    Methoden im Erkennungsalgorhitmus, z.B. das Verkleinern der einzelnen Bits auf ihren mittleren Kern, sind in diesem Fall nicht mehr effektiv. 
    \ref{fig:figure21} erweckt bei genauer Betrachtung den Eindruck, dass sich die VErzerrung kompensieren lassen müsste. 
    Zwar lässt sich für diesen speziellen Fall eine Funktion schreiben, die den Marker zumindest soweit korrigiert, dass er als korrekter Marker erkannt wird, jedoch baut diese Funktion
    auf eine manuelle Ermittlung der Koordinaten der Bitmatrix auf. 
    Die Verwendung der Ausgleichspolynome bei anderen Markern führt zu keiner erhöhten Erkennungsrate. 

    \begin{figure}
        \caption[Erkennungsrate der Becher ohne Bildbearbeitung]{\small Erkennungsrate der Becher ohne Bildbearbeitung.}\label{fig:figure22}
        \includegraphics[width = \textwidth]{Bilder/ErkennungsrateOBB.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Erkennungsrate der Becher mit Gauß 5x5]{\small Erkennungsrate der Becher nach Gauß 5x5 Filterung.}\label{fig:figure23}
        \includegraphics[width = \textwidth]{Bilder/ErkennungsrateGauss.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Erkennungsrate der BEcher mit Laplace Filter]{\small Erkennungsrate der Becher nach Laplace Filterung.}\label{fig:figure24}
        \includegraphics[width = \textwidth]{Bilder/ErkennungsrateLaplace.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Erkennungsrate der Becher mit Gauß 5x5 und Laplace Filter]{\small Erkennungsrate der Becher nach Gauß 5x5 und Laplace Filterung (gewichtete Überlagerung).}\label{fig:figure25}
        \includegraphics[width = \textwidth]{Bilder/ErkennungsrateGausLaplace.png}
        \centering
    \end{figure}

    \begin{figure}
        \caption[Erkennungsrate der Becher nach Schärfung und Gauß 5x5]{Erkennungsrate der Becher nach Schärfung und anschließender Gauß 5x5 Filterung}\label{fig:figure26}
        \includegraphics[width = \textwidth]{Bilder/ErkennungsrateScharfGauss.png}
        \centering
    \end{figure}

    Es wurden Versuche unternommen um die Erkennung mit systematischen Methoden zu verbessern.
    In \ref{fig:figure22} ist die Erkennungsrate der Becher ohne Bildbearbeitung dargestellt und dient somit als Referenz für die darauffolgenden Methoden der Bildverarbeitung.
    Es wurden 4 Marker korrekt erkannt, was durch das grüne Viereckt und der roten ID- Notiz erkennbar ist. 
    Es wurden zudem richtigerweise in den Plätzen L7, L11, L13, L14, L15 und L16 keine Becher erkannt.
    In \ref{fig:figure23} ist die Erkennungsrate nach einer Gauß 5x5 Filterung dargestellt. 
    Die Überlegung ist, dass durch das Reduzieren von Rauschen die Umwandlung von Graubild zu Binärbild dafür sorgt, dass die Binärmatrix gut genug dekodiert werden kann. 
    Wie in der Abbildung ersichtlich ist dies für L10 sogar der Fall - ansonsten ist jedoch keine Änderung des Ergebnisses ersichtlich.
    Der Versuch die Erkennung mit einem Laplace Kantenfilter zu verbessern, ist in \ref{fig:figure24} dargestellt und ergibt sogar eine schlechtere Erkennungsrate als das Referenzbild.
    In \ref{fig:figure25} ist die Erkennungsrate nach einer gewichteten Überlagerung von Gauß 5x5 und Laplace Filterung dargestellt. 
    Dazu wird zunächst ein mit einem Gauß 5x5 Filter gefiltertes Bild erzeugt und mit einem Laplace-Kantenbild überlagert. 
    Für die Überlagerung wird die Funktion \verb|cv2.addWeighted()| verwendet. 
    Durch die Überlagerung ergibt sich theoretisch ein höherer Kontrast der Kanten in der Binärmatrix. 
    Die Erkennungsrate ist jedoch nicht besser als das Referenzbild.
    In \ref{fig:figure26} ist die Erkennungsrate nach einer Schärfung und anschließender Gauß 5x5 Filterung dargestellt.
    Für die Schärfung wurde die Filtermatrix $W$ verwendet, die mit 
    \begin{align*}
        W = \begin{bmatrix}
            -1 & -1 & -1 \\
            -1 & 9 & -1 \\
            -1 & -1 & -1
        \end{bmatrix}
    \end{align*}
       

    definiert ist. Allein das Schärfen des Bildes brachte keinen Erfolg. Nach der Gauß-Filterung steigt die Erkennungsrate auf 8 Marker an. 
    Die Marker L9, L10 und L18 werden nicht erkannt, was das Gesamtergebnis immer noch als zu unbefriedigend erscheinen lässt.
    Als Fazit der Versuche lässt sich festhalten, dass die Erkennung der Becher nicht mit einer zufriedenstellenden Sicherheit durch die verwendete Übersichtskamera durchführen lässt. 

    \subsubsection{Erkennungsrate der ESP32 Kamera}

    Die ESP32 Kamera hat eine deutlich geringere Auflösung als die $\mu$Eye Kamera, hat aber einen sehr niedrigen Mindestabstand der Appertur.
    Um die Eignung der Kamera zu zeigen wurden 3 Testbilder aufgenommen, die in \ref{fig:figure27} dargestellt sind.
    Hier zeigt sich der Vergleich zu den vorangegangenen Abschnitten, dass die Erkennung der Marker selbst mit Wölbung kein Problem ist, wenn der Bildbereich und die Auflösung in einem günstigen Verhältnis stehen.
    Ohne weitere Bildverarbeitung werden die Marker im linken und rechten Bereich erkannt. 
    Das rechte Bild zeigt, dass bei der Anbringung und Größe der Marker grenzen gesetzt sind: Wenn der Rand der Binärmatrix außerhalb des Bildbereichs ist, wird der Marker nicht erkannt.

    \begin{figure}
        \caption[Testbilder mit ESP 32 WebCam]{Testbilder im Zuge der Positionsermittlugn für die ESP32 Kamera auf dem Greifer des IRB 140 Roboterarms.}\label{fig:figure27}
        \includegraphics[width = \textwidth]{Bilder/ESP32CamTest.png}
        \centering
    \end{figure}

    \subsubsection{Festlegung der Markergröße und Position}

    Die Marker werden in folgende Gruppen unterschieden:
    \begin{itemize}
        \item Regalmarker
        \item Palettenmarker
        \item Bechermarker
    \end{itemize}
    Die größe der Regalmarker hat sich mit 30x30mm bewährt, die Erkennungsrate ist ausreichend. die Wenigen Fehlschläge können in der Software abgefangen werden und eine neue Aufnahme anfordern.
    Die Palettenmarker haben sind mit einer größe von 30x30mm ebenfalls ausreichend. Die Erkennnung wird teilweise durch den Greifer verhindert, wenn dieser in Grundstellung ist. 
    Für die Palettenerkennung muss der Roboterarm daher die Grundstellung verlassen. Eine Vergrößerung der Marker birgt das Risiko, dass Markerelemente den Bildbereich oder den Segmentbereich verlassen. 
    Eine Kleinere Markergröße verschlechtert die Erkennungsrate mit der Übersichtskamera enorm. In den obigen Bildern wurde bspw. L18 nicht ein Mal erkannt.
    
    Die Erkennung der Becher mittels Übersichtkamera wird durch zwei Aspekte verhindert:
    \begin{itemize}
        \item Die Wölbung der Becher erschwert das Auslesen in Abhängigkeit von der Winkelposition des Bechers zur Kamera.
        \item Die vorderen Becher verdecken die hinteren Becher.
    \end{itemize}

    Daher erfolgt die Erkennung der Becher mittels ESP32 Kamera auf dem Greifer des Roboterarms.
    Günstige Gelegenheiten ohne nennenswerte Störung des Betriebsablaufs sind gegeben, bevor der Roboter einen Becher oder Palette greift.
    In diesen Situationen blickt die Kamera mittig und rechtwinklig auf den Bechermantel, wie \ref{fig:figure27} veranschaulicht.

    Um die Erkennnung der Marker zu gewährleisten, muss um den Rand des Markers ein \glqq Ruhebereich \grqq von wenigen Millimetern eingehalten werden.
    Dies trifft insbesondere auf die Palettenmarker zu, da die Paletten ein dunkles Grau haben und die Unterscheidung zum Marker bei ungünstigen Lichtverhältnissen unmöglich macht.
    Das Drucken der Marker mit schwarzem Toner auf weißem Druckerpapier ist ausreichend.
